{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QrJRFzQQ1puL"
   },
   "source": [
    "# LSTM Experiment\n",
    "\n",
    "This notebook is an entry point to reproduce the experiments describe in the LSTM section of the paper.\n",
    "\n",
    "It consists of three sections:\n",
    "\n",
    "    1) Data Loading\n",
    "    2) Example on how to train and test a LSTM on our data\n",
    "    3) The actual experiment code from the paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x1u7mm60Xjw5"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ak1rmTI8YIey"
   },
   "outputs": [],
   "source": [
    "# For use in google colab\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oLsuMMvvXjxQ"
   },
   "outputs": [],
   "source": [
    "import DataSet\n",
    "import Evaluation\n",
    "import datetime\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from Utils import *\n",
    "\n",
    "from DataSet import UniHHIMUGestures\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dqm2OrAXYLfA"
   },
   "outputs": [],
   "source": [
    "def getProjectPath():\n",
    "    return '/.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SnSwJOLXjxi"
   },
   "source": [
    "We need to specify a bunch of parameters the expriment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14Ptf_AhXjxo"
   },
   "outputs": [],
   "source": [
    "#===========================================================================\n",
    "# Give this run a name. \n",
    "# If name equals 'test', no log will be generated\n",
    "#===========================================================================\n",
    "name = 'test'\n",
    "\n",
    "\n",
    "#===========================================================================\n",
    "# Decide which gesture data shall be used for training\n",
    "#===========================================================================\n",
    "inputGestures = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "#===========================================================================\n",
    "# Decide which target signals shall be used for training\n",
    "#===========================================================================\n",
    "usedGestures = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "#===========================================================================\n",
    "# Concatenate data to create \"more\" training samples, 1 corresponds to no concatenations\n",
    "#===========================================================================\n",
    "concFactor = 1\n",
    "\n",
    "#===========================================================================\n",
    "# Add noise to the data, 0 corresponds to no noise. Noise above 2 has shown to weaken recognition\n",
    "#===========================================================================\n",
    "noiseFactor = 1\n",
    "\n",
    "#===========================================================================\n",
    "# Decide wether gestures shall be shuffled before training. If true, nFolds many \n",
    "# pieces will be generated. Not every piece is garanteed to contain every gesture, so do not use too many.\n",
    "#===========================================================================\n",
    "shuffle = True\n",
    "nFolds = 4\n",
    "\n",
    "\n",
    "#===========================================================================\n",
    "# Function used to evaluate during cross validation. Possible functions are:\n",
    "# Evaluation.calc1MinusF1FromMaxApp (best working, used in thesis)\n",
    "# Oger.utils.nmse (normalised mean square error, tells nothing about classifier perfomance but works okay)\n",
    "# Evaluation.calcLevenshteinError (use the Levenshtein error, disadvantages are highlighted in thesis) \n",
    "# Evaluation.calc1MinusF1FromInputSegment (use segmentation by supervised signal)\n",
    "#===========================================================================\n",
    "evaluationFunction = Evaluation.calc1MinusF1FromMaxApp\n",
    "\n",
    "#===========================================================================\n",
    "# Set this to true if another output neuron shall be added to represent \"no gesture\"\n",
    "#===========================================================================\n",
    "learnTreshold = False\n",
    "\n",
    "#===========================================================================\n",
    "# Use on of the optimisation dictionaries from the optDicts file\n",
    "#===========================================================================\n",
    "optDict = 'bestParas'\n",
    "\n",
    "#===========================================================================\n",
    "# Use normalizer\n",
    "#===========================================================================\n",
    "useNormalized = 2\n",
    "\n",
    "#===========================================================================\n",
    "# Pick datasets to train on, and datasets to test on\n",
    "#===========================================================================\n",
    "inputFiles = ['nike','julian','nadja','line']\n",
    "testFiles = ['stephan']\n",
    "\n",
    "# If desired add a specific file to test on, e.g. randTestFiles = ['lana_0_0.npz']\n",
    "randTestFiles = []\n",
    "\n",
    "\n",
    "\n",
    "#===========================================================================\n",
    "# Setup project directory\n",
    "#===========================================================================\n",
    "now = datetime.datetime.now()\n",
    "resultsPath = getProjectPath()+'results/'\n",
    "pdfFileName = now.strftime(\"%Y-%m-%d-%H-%M\")+'_'+name+'.pdf'\n",
    "pdfFilePath = resultsPath+'pdf/'+pdfFileName\n",
    "npzFileName = now.strftime(\"%Y-%m-%d-%H-%M\")+'_'+name+'.npz'\n",
    "npzFilePath = resultsPath+'npz/'+npzFileName\n",
    "bestFlowPath = resultsPath+'nodes/'+now.strftime(\"%Y-%m-%d-%H-%M\")+'_'+name+'.p'\n",
    "pp = PdfPages(pdfFilePath)\n",
    "\n",
    "\n",
    "#===========================================================================\n",
    "# Add labels for gestures\n",
    "#===========================================================================\n",
    "totalGestureNames = ['left','right','forward','backward','bounce up','bounce down',\n",
    "                     'turn left','turn right','shake lr','shake ud',\n",
    "                     'tap 1','tap 2','tap 3','tap 4','tap 5','tap 6','no gesture']\n",
    "gestureNames = []\n",
    "for i in usedGestures:\n",
    "    gestureNames.append(totalGestureNames[i])\n",
    "gestureNames.append('no gesture')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-r4Ua_FmXjx1"
   },
   "source": [
    "Create dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1CKFlNaXjx5"
   },
   "outputs": [],
   "source": [
    "def createData(inputFiles, testFiles, validation_size=10):\n",
    "    trainset_ = UniHHIMUGestures(dataDir=getProjectPath() + 'dataSets/', \n",
    "                                train=True, \n",
    "                                inputFiles=inputFiles,\n",
    "                                testFiles=testFiles,\n",
    "                                useNormalized=useNormalized, \n",
    "                                learnTreshold=learnTreshold,\n",
    "                                nFolds=100,\n",
    "                                shuffle=True,\n",
    "                               )\n",
    "\n",
    "    testset = UniHHIMUGestures(dataDir=getProjectPath() + 'dataSets/', \n",
    "                               train=False, \n",
    "                               inputFiles=inputFiles,\n",
    "                               testFiles=testFiles,\n",
    "                               useNormalized=useNormalized, \n",
    "                               learnTreshold=learnTreshold,\n",
    "                               shuffle=True\n",
    "                              )\n",
    "    \n",
    "    trainset, validationset = torch.utils.data.random_split(trainset_, [len(trainset_)-validation_size,validation_size])\n",
    "    \n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=1,\n",
    "                            shuffle=True, num_workers=1)\n",
    "    validationloader = DataLoader(validationset, batch_size=1,\n",
    "                                shuffle=True, num_workers=1)\n",
    "    testloader = DataLoader(testset, batch_size=1,\n",
    "                            shuffle=True, num_workers=1)\n",
    "    return trainset, validationset, testset, trainloader, validationloader, testloader\n",
    "    \n",
    "trainset, validationset, testset, trainloader, validationloader, testloader = createData(inputFiles, testFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9GCQCtfOXjyH"
   },
   "source": [
    "Let's take a look at the scaled input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wOes7SOnXjyK"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1, figsize=(15,9))\n",
    "ax[0].plot(trainset[0][0][:800,0:3])\n",
    "ax[1].plot(trainset[0][0][:800,3:6])\n",
    "ax[2].plot(trainset[0][0][:800,6:9])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f272FhRMXjyj"
   },
   "source": [
    "Looks all good, we can clearly see the gesture sequences intercepted by non gesture seqeuences in between.\n",
    "Now let's create a Leaky Integrator ESN with parameters as defined in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPi2g5rsXj20"
   },
   "source": [
    "# LSTM\n",
    "\n",
    "This section shows how to create and train LSTM on the give data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHEvdCxdXj3c"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=25):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(input_size=9, hidden_size=hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim,10, bias=False),\n",
    "            #torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lstm_outputs, (hidden_acts, cell_states) = self.lstm(inputs)\n",
    "        \n",
    "        predictions = self.classifier(lstm_outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LXVYixr7Xj3p"
   },
   "outputs": [],
   "source": [
    "lstm = LSTMClassifier(20)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=1e-3)\n",
    "\n",
    "lstm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5n8a9bqXj4P"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "\n",
    "def testLSTM(testloader, lstm, plot=True, plotConf=False):\n",
    "    testF1MaxApps = []\n",
    "    testAccuracies = []\n",
    "    testCms = []\n",
    "    losses = []\n",
    "    for inputs, targets in testloader:\n",
    "        test_inputs, test_targets = inputs.float(), targets.float()\n",
    "        \n",
    "        #inputs[0,:,:9] = targets[0,:,:9]\n",
    "        test_inputs = (test_inputs/test_inputs.std(1))\n",
    "        \n",
    "        test_inputs = test_inputs.cuda()\n",
    "        test_targets = test_targets.cuda()\n",
    "\n",
    "        lstm.eval()\n",
    "\n",
    "\n",
    "        outputs = lstm(test_inputs)\n",
    "        loss = loss_function(outputs, test_targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        test_preds = outputs.cpu().detach().numpy()[0]\n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=(15,5))\n",
    "            plt.plot(test_preds[:1600,:])\n",
    "            plt.plot(test_targets[0,:1600,:].cpu())\n",
    "            plt.pause(1)\n",
    "        \n",
    "        fixed_threshold = 0.4\n",
    "\n",
    "        t_target = test_targets[0].cpu().numpy()\n",
    "        prediction = test_preds[:,:10]\n",
    "        if learnTreshold: # if threshold is learned, then it's the las collumn of the prediction\n",
    "            threshold = outputs[0].numpy()[:,10]\n",
    "        else: #else add a constant threshold\n",
    "            threshold = np.ones((prediction.shape[0],1))*fixed_threshold\n",
    "\n",
    "        t_maxApp_prediction = Evaluation.calcMaxActivityPrediction(prediction,t_target,threshold, 10)\n",
    "\n",
    "\n",
    "        pred_MaxApp, targ_MaxApp = Evaluation.calcInputSegmentSeries(t_maxApp_prediction, t_target, 0.5)\n",
    "        testF1MaxApps.append(np.mean(sklearn.metrics.f1_score(targ_MaxApp,pred_MaxApp,average=None)))\n",
    "        testAccuracies.append(np.mean(sklearn.metrics.accuracy_score(targ_MaxApp,pred_MaxApp)))\n",
    "\n",
    "        if False:\n",
    "        #print(t_maxApp_prediction.shape, prediction.shape, pred_MaxApp, targ_MaxApp)\n",
    "            plt.figure(figsize=(20,3))\n",
    "            plt.plot(t_maxApp_prediction[:800])\n",
    "            plt.plot(t_target[:800])\n",
    "\n",
    "\n",
    "        conf = sklearn.metrics.confusion_matrix(targ_MaxApp, pred_MaxApp)\n",
    "        testCms.append(conf)\n",
    "\n",
    "        if plotConf:\n",
    "            Evaluation.plot_confusion_matrix(testCms[0], gestureNames, 'test set')\n",
    "            plt.tight_layout()\n",
    "            plt.ylim(10.5,-0.5)\n",
    "\n",
    "    #print(\"Test f1 score for maxactivity: {:.4f}, MSE: {:.4f}\".format(np.mean(testF1MaxApps),loss.item()))\n",
    "\n",
    "    return np.mean(testF1MaxApps), np.mean(losses), np.mean(testAccuracies), testCms\n",
    "\n",
    "testLSTM(testloader, lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhmcQw_IXj31",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "def trainLSTM(inputFiles, testFiles, plot=True):\n",
    "    trainset, validationset, testset, trainloader, validationloader, testloader = createData(inputFiles, testFiles)\n",
    "\n",
    "    \n",
    "    lstm = LSTMClassifier(80)\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    optimizer = optim.Adam(lstm.parameters(), lr=1e-3)\n",
    "\n",
    "    lstm.cuda()\n",
    "\n",
    "    best_score = 0.\n",
    "    best_model = lstm.state_dict()\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_scores = []\n",
    "\n",
    "    for epoch in range(100):\n",
    "        epoch_losses = []\n",
    "        for inputs, targets in trainloader:\n",
    "            lstm.train()\n",
    "            inputs, targets = inputs.float(), targets.float()\n",
    "            \n",
    "            #inputs[0,:,:9] = targets[0,:,:9]\n",
    "            inputs = (inputs/inputs.std(1))\n",
    "            \n",
    "            inputs = inputs[:,:,:].cuda()\n",
    "            targets = targets[:,:,:].cuda()\n",
    "\n",
    "            lstm.zero_grad()\n",
    "\n",
    "            outputs =  lstm(inputs)\n",
    "\n",
    "\n",
    "            loss = loss_function(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "        train_losses.append(epoch_losses)\n",
    "\n",
    "        score, mse, accuracies, cm = testLSTM(validationloader, lstm, plot=False)\n",
    "        test_losses.append(mse.item())\n",
    "        test_scores.append(score.item())\n",
    "\n",
    "        if score > best_score:\n",
    "            best_model = copy.deepcopy(lstm.state_dict())\n",
    "            torch.save(lstm.state_dict(), 'weights_only.pth')\n",
    "            best_score = score\n",
    "        if score + 0.1 < best_score:\n",
    "            print('Overfitted, breaking the loop')\n",
    "            break\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            print(loss.item())\n",
    "            lstm.eval()\n",
    "            plt.figure(figsize=(15,5))\n",
    "            plt.title('Epoch: {:}, loss: {:.4f}'.format(epoch, loss))\n",
    "            #test_preds = lstm(test_inputs.float()).detach().numpy()[0]\n",
    "            #plt.plot(test_preds[:800,:])\n",
    "            #plt.plot(test_targets[0,:800,:])\n",
    "            preds = lstm(inputs)\n",
    "            if plot:\n",
    "                plt.plot(preds[0,:800].cpu().detach())\n",
    "                plt.plot(targets[0,:800].cpu().detach())\n",
    "                plt.ylim(-0.1,1.1)\n",
    "                plt.pause(1)\n",
    "            score, mse, accuracies, cm = testLSTM(validationloader,lstm,  plot=False)\n",
    "            print(\"Best score: {:.2f}, current score: {:.2f}\".format(best_score, score, mse))\n",
    "\n",
    "\n",
    "    plt.plot([np.mean(epoch_losses) for epoch_losses in train_losses])\n",
    "    plt.plot([x for x in test_losses])\n",
    "    \n",
    "    lstm.load_state_dict(best_model)\n",
    "\n",
    "    train_score, train_mse, train_accuracy, train_cm = testLSTM(trainloader, lstm,  plot=False, plotConf=False)\n",
    "    score, mse, accuracy, cm = testLSTM(testloader, lstm,  plot=plot, plotConf=plot)\n",
    "    print(\"Best score: {:.2f}, current score: {:.2f}\".format(best_score, score, mse))\n",
    "\n",
    "    print('###################################################################')\n",
    "\n",
    "    return score, mse, accuracy, cm, train_score, train_mse, train_accuracy, lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4-cQ19lJHdL"
   },
   "outputs": [],
   "source": [
    "inputFiles = ['stephan','nike','nadja','line']\n",
    "testFiles = ['julian']\n",
    "\n",
    "scores = []\n",
    "\n",
    "\n",
    "\n",
    "for trial in range(1):\n",
    "    print('################# TRIAL: {} #########################'.format(trial))\n",
    "    score, mse, accuracy, cm, train_score, train_mse, train_accuracy, lstm = trainLSTM(inputFiles=inputFiles, testFiles=testFiles, plot=True)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bvBfSWFkXj38"
   },
   "outputs": [],
   "source": [
    "best_lstm = LSTMClassifier(80)\n",
    "best_lstm.load_state_dict(torch.load('weights_only.pth'))\n",
    "best_lstm.cuda().eval()\n",
    "\n",
    "testLSTM(testloader, best_lstm, plot=True, plotConf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMit19iLXj4D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sz_Hm0RcXj4V"
   },
   "outputs": [],
   "source": [
    "score, mse, accuracies, cms = testLSTM(testloader, best_lstm,  plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVDpQFllWCYW"
   },
   "source": [
    "Scores for 125 epochs of training on nadja, mean: 0.9143460460123742 (0.05), \n",
    "scores:\n",
    "[0.8181441063473731,\n",
    " 0.9231134101237749,\n",
    " 0.9137734105891174,\n",
    " 0.9391169931097335,\n",
    " 0.9671708916560177,\n",
    " 0.9419715272829966,\n",
    " 0.8996514255373884,\n",
    " 0.8261919621187356,\n",
    " 0.9544290335413912,\n",
    " 0.9598976998172137]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment from paper\n",
    "\n",
    "In this experiment we train an LSTM on four of the testsets and evaluate on the fith one. Training and evaluation is repeated several times to average the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q4EGI3yW92Lq"
   },
   "outputs": [],
   "source": [
    "# Prepare arrays to store evaualtion results\n",
    "\n",
    "files = ['stephan','julian','nadja','line','nike']\n",
    "\n",
    "trials = 10\n",
    "\n",
    "if False:\n",
    "    all_scores = np.zeros((len(files),trials))\n",
    "    all_train_scores = np.zeros((len(files),trials))\n",
    "    all_cms = np.zeros((len(files),trials, 11, 11))\n",
    "    all_accuracies = np.zeros((len(files),trials))\n",
    "    all_train_accuracies = np.zeros((len(files),trials))\n",
    "\n",
    "    np.save(file=getProjectPath()+\"lstm_scores\", arr=np.array(all_scores))\n",
    "    np.save(file=getProjectPath()+\"lstm_train_scores\", arr=np.array(all_train_scores))\n",
    "    np.save(file=getProjectPath()+\"lstm_cms\", arr=np.array(all_cms))\n",
    "    np.save(file=getProjectPath()+\"lstm_accuracies\", arr=np.array(all_accuracies))\n",
    "    np.save(file=getProjectPath()+\"lstm_train_accuracies\", arr=np.array(all_train_accuracies))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pd2fVLrPREzx"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    all_scores = np.load(\"lstm_scores.npy\")\n",
    "    all_train_scores = np.load(\"lstm_train_scores.npy\")\n",
    "    all_cms = np.load(\"lstm_cms.npy\")\n",
    "    all_accuracies = np.load(\"lstm_accuracies.npy\")\n",
    "    all_train_accuracies = np.load(\"lstm_train_accuracies.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oyfjog6-UgoC"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def fix_seed(manualSeed):\n",
    "    \n",
    "    np.random.seed(manualSeed)\n",
    "    random.seed(manualSeed)\n",
    "    torch.manual_seed(manualSeed)\n",
    "    # if you are using GPU\n",
    "    torch.cuda.manual_seed(manualSeed)\n",
    "    torch.cuda.manual_seed_all(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1W2d0Pj9_AS"
   },
   "outputs": [],
   "source": [
    "# run evaluation for each testset\n",
    "\n",
    "fix_seed(1)\n",
    "\n",
    "start_ind=0\n",
    "\n",
    "for idx in tqdm_notebook(range(int(start_ind/trials) ,5)):\n",
    "    scores = []\n",
    "    train_scores = []\n",
    "    accuracies = []\n",
    "    networks = []\n",
    "    \n",
    "    # Shuffle testsets\n",
    "    inputFiles = files[:idx] + files[idx+1:]\n",
    "    testFiles = files[idx:idx+1]\n",
    "\n",
    "\n",
    "    for t in tqdm_notebook(range(int(start_ind%trials),trials)):\n",
    "        print('Exp id:', idx, t)\n",
    "        score, mse, accuracy, cm, train_score, train_mse, train_accuracy, lstm = \\\n",
    "            trainLSTM(inputFiles=inputFiles, testFiles=testFiles, plot=False)\n",
    "\n",
    "        all_scores[idx,t] = score\n",
    "        all_train_scores[idx,t] = train_score\n",
    "        all_accuracies[idx,t] = accuracy\n",
    "        all_train_accuracies[idx,t] = train_accuracy\n",
    "        all_cms[idx,t] = cm[0]\n",
    "\n",
    "\n",
    "        #np.save(file=getProjectPath()+\"lstm_scores\", arr=np.array(all_scores))\n",
    "        #np.save(file=getProjectPath()+\"lstm_train_scores\", arr=np.array(all_train_scores))\n",
    "        #np.save(file=getProjectPath()+\"lstm_cms\", arr=np.array(all_cms))\n",
    "        #np.save(file=getProjectPath()+\"lstm_accuracies\", arr=np.array(all_accuracies))\n",
    "        #np.save(file=getProjectPath()+\"lstm_train_accuracies\", arr=np.array(all_train_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0-ufW_2-Blc"
   },
   "outputs": [],
   "source": [
    "# print results\n",
    "import pandas as pd\n",
    "pd.DataFrame(np.vstack([\n",
    "        all_accuracies.mean(1),\n",
    "        all_accuracies.std(1),\n",
    "        all_train_accuracies.mean(1),\n",
    "        all_train_accuracies.std(1),\n",
    "        all_scores.mean(1),\n",
    "        all_scores.std(1),\n",
    "        all_train_scores.mean(1),\n",
    "        all_train_scores.std(1),\n",
    "    ]), \n",
    "    columns=files, \n",
    "    index=['accuracies','accuracies_std','accuracies_train','accuracies_train_std','f1','f1_std','f1_train','f1_train_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWokpPSORNwc"
   },
   "outputs": [],
   "source": [
    "all_scores.mean(), all_scores.std(), all_accuracies.mean(), all_accuracies.std(), all_train_scores.mean(), all_train_scores.std(),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CCfmb0Fz-1Fj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uUq0-EUe-5O1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "at8ftn-c-_2P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gzuAYiao-IhS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uF-fHi3o_E3_"
   },
   "outputs": [],
   "source": [
    "score, mse, accuracy, cm, train_score, train_mse, train_accuracy, lstm = \\\n",
    "            trainLSTM(inputFiles=inputFiles, testFiles=testFiles, plot=True)\n",
    "\n",
    "#testLSTM(testloader,lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QnlsEReoMUn7"
   },
   "outputs": [],
   "source": [
    "trainset, validationset, testset, trainloader, validationloader, testloader = createData(inputFiles, testFiles)\n",
    "score, mse, accuracy, cm = testLSTM(testloader, lstm,  plot=True, plotConf=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IzjuJ5YJAcAg"
   },
   "outputs": [],
   "source": [
    "# plot median confustion matrices\n",
    "\n",
    "from Evaluation import plot_confusion_matrix\n",
    "\n",
    "\n",
    "median_cms = []\n",
    "for i, (testfile, scores, cms) in enumerate(zip(files, all_scores, all_cms)):\n",
    "    print(i, testfile)\n",
    "    # get median model\n",
    "    median_idx = np.argsort(scores)[round(len(scores)/2)]\n",
    "    median_idx.argsort\n",
    "    median_score = scores[median_idx]\n",
    "    median_cm = cms[median_idx]\n",
    "    median_cms.append(median_cm)\n",
    "    fig = plot_confusion_matrix(median_cm.astype('int'), gestures=totalGestureNames[:10] + totalGestureNames[-1:])\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    pp = PdfPages('figures/lstm_experiment_{}_f1_score_{:.2f}.pdf'.format(testfile, median_score))\n",
    "    pp.savefig()\n",
    "    pp.close()\n",
    "    plt.savefig('figures/lstm_experiment_{}_f1_score_{:.2f}.eps'.format(testfile, median_score), format='eps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbf-Ee7yA0Uk"
   },
   "outputs": [],
   "source": [
    "# plot average confusion matrices\n",
    "\n",
    "from Evaluation import plot_confusion_matrix\n",
    "\n",
    "\n",
    "median_cms = []\n",
    "for i, (testfile, scores, cms) in enumerate(zip(files, all_scores, all_cms)):\n",
    "    print(i, testfile)\n",
    "    # get median model\n",
    "    median_idx = np.argsort(scores)[round(len(scores)/2)]\n",
    "    median_idx.argsort\n",
    "    median_score = scores[median_idx]\n",
    "    median_cm = cms[median_idx]\n",
    "    median_cms.append(median_cm)\n",
    "    fig = plot_confusion_matrix(cms.mean(0), gestures=totalGestureNames[:10] + totalGestureNames[-1:])\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    pp = PdfPages('figures/lstm_experiment_{}_avg__f1_score_{:.2f}.pdf'.format(testfile, scores.mean()))\n",
    "    pp.savefig()\n",
    "    pp.close()\n",
    "    plt.savefig('figures/lstm_experiment_{}_avg_f1_score_{:.2f}.eps'.format(testfile, scores.mean()), format='eps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1AV42D4_MvU"
   },
   "outputs": [],
   "source": [
    "inputFiles = [ 'julian', 'nadja', 'line', 'nike']\n",
    "testFiles = ['stephan']\n",
    "trainset, validationset, testset, trainloader, validationloader, testloader = createData(inputFiles, testFiles)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(testset[0][1][:,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xelq5D_JCL84"
   },
   "outputs": [],
   "source": [
    "# create activations plot\n",
    "\n",
    "import matplotlib\n",
    "old_font = matplotlib.rcParams.get('font.size')\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "    \n",
    "\n",
    "for inputs, targets in testloader:\n",
    "    test_inputs, test_targets = inputs.float(), targets.float()\n",
    "    \n",
    "    #inputs[0,:,:9] = targets[0,:,:9]\n",
    "    test_inputs = (test_inputs/test_inputs.std(1))\n",
    "    \n",
    "    test_inputs = test_inputs.cuda()\n",
    "    test_targets = test_targets.cuda()\n",
    "\n",
    "    lstm.eval()\n",
    "\n",
    "\n",
    "    outputs = lstm(test_inputs)\n",
    "    loss = loss_function(outputs, test_targets)\n",
    "    #losses.append(loss.item())\n",
    "\n",
    "    test_preds = outputs.cpu().detach().numpy()[0]\n",
    "\n",
    "    if True:\n",
    "        plt.figure(figsize=(15,6))\n",
    "        for signal, target, name in zip(test_preds.T,test_targets[0].T, gestureNames):\n",
    "            plt.plot(signal[0:480], label=name)\n",
    "            plt.fill_between(np.arange(480), np.zeros(480), target[0:480,].cpu(), alpha=0.05)\n",
    "        plt.plot(test_targets[0,0:480,:].cpu())\n",
    "        plt.xlim(0,700)\n",
    "        #plt.pause(1)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Timesteps')\n",
    "        plt.ylabel('Activation')\n",
    "    \n",
    "    fixed_threshold = 0.4\n",
    "\n",
    "    t_target = test_targets[0].cpu().numpy()\n",
    "    prediction = test_preds[:,:10]\n",
    "    if learnTreshold: # if threshold is learned, then it's the las collumn of the prediction\n",
    "        threshold = outputs[0].numpy()[:,10]\n",
    "    else: #else add a constant threshold\n",
    "        threshold = np.ones((prediction.shape[0],1))*fixed_threshold\n",
    "\n",
    "    t_maxApp_prediction = Evaluation.calcMaxActivityPrediction(prediction,t_target,threshold, 10)\n",
    "\n",
    "\n",
    "    pred_MaxApp, targ_MaxApp = Evaluation.calcInputSegmentSeries(t_maxApp_prediction, t_target, 0.5)\n",
    "    #testF1MaxApps.append(np.mean(sklearn.metrics.f1_score(targ_MaxApp,pred_MaxApp,average=None)))\n",
    "    #testAccuracies.append(np.mean(sklearn.metrics.accuracy_score(targ_MaxApp,pred_MaxApp)))\n",
    "\n",
    "    if False:\n",
    "    #print(t_maxApp_prediction.shape, prediction.shape, pred_MaxApp, targ_MaxApp)\n",
    "        plt.figure(figsize=(20,3))\n",
    "        plt.plot(t_maxApp_prediction[:800])\n",
    "        plt.plot(t_target[:800])\n",
    "\n",
    "\n",
    "    conf = sklearn.metrics.confusion_matrix(targ_MaxApp, pred_MaxApp)\n",
    "    #testCms.append(conf)\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': old_font})\n",
    "    \n",
    "pp = PdfPages('figures/subgesture.pdf'.format(testfile, scores.mean()))\n",
    "pp.savefig()\n",
    "pp.close()\n",
    "plt.savefig('figures/subgesture.eps'.format(testfile, scores.mean()), format='eps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E4qOizRRr1T6"
   },
   "outputs": [],
   "source": [
    "plt.plot(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dXAxV-4gtKCs"
   },
   "outputs": [],
   "source": [
    "(targets[0,:-1] < targets[0,1:]).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGMOEekLtPt7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Doreen Paper Main LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
